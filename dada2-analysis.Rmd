---
title: "supernova 16s Notebook"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

```{r Setup Packages}
library(dada2)
library(decontam)
library(ggplot2)
library(jsonlite)
library(lattice)
library(lemon)
library(reshape)
library(zoo)
knit_print.data.frame <- lemon_print
PROJECT_DIR = "/home/rpetit/projects/supernova-16s"
setwd(PROJECT_DIR)
getN <- function(x) sum(getUniques(x))
```

# Input FASTQs
Input FASTQs had adapters removed using BBDuk prior to DADA2 analysis. This was accomplished with the following command:
```{bash eval=FALSE, include=FALSE}
cd /home/rpetit/projects/supernova-16s
ls data/fastq/ | \
    grep "R1" | \
    sed 's/_L001_R1_001.fastq.gz//' | \
    awk '{print $1" data/fastq/"$1"_L001_R1_001.fastq.gz data/fastq/"$1"_L001_R2_001.fastq.gz data/adapter-cleaned"}' | \
    xargs -I {} sh -c 'scripts/remove-adapters.sh {}'
```

The adapter cleaned FASTQ files were stored in `data/adapter-cleaned` 

# Dada2 Analysis
## Setting Up Files and Naming
```{r Setup Naming}
# Input FASTQs
fastq_path = paste0(PROJECT_DIR, "/data/adapter-cleaned")
fastq_r1 <- sort(
  list.files(fastq_path, pattern="_R1.fastq.gz", full.names = TRUE)
)
fastq_r2 <- sort(
  list.files(fastq_path, pattern="_R2.fastq.gz", full.names = TRUE)
)
sample_names <- sapply(strsplit(basename(fastq_r1), "_"), `[`, 1)
```

## Plot Project FASTQ Quality Profile
```{r Project Quality Profile, fig.width=12, fig.asp=0.5}
json_r1 <- sort(list.files(fastq_path, pattern="_R1.json", full.names = TRUE))
json_r2 <- sort(list.files(fastq_path, pattern="_R2.json", full.names = TRUE))
r1_profile = NULL
r2_profile = NULL
for (i in 1:length(json_r1)){
  r1_df = as.data.frame(fromJSON(txt=json_r1[i])$per_base_quality)
  r2_df = as.data.frame(fromJSON(txt=json_r2[i])$per_base_quality)
  if (is.null(r1_profile)) {
    r1_profile = r1_df
    r2_profile = r2_df
  }
  else {
    r1_profile = rbind(r1_profile, r1_df)
    r2_profile = rbind(r2_profile, r2_df)
  }
}

colnames(r1_profile) <- as.integer(gsub("X", "", colnames(r1_profile)))
colnames(r2_profile) <- as.integer(gsub("X", "", colnames(r2_profile)))
means = data.frame(
  position=1:ncol(r1_profile), 
  R1=as.data.frame(colMeans(r1_profile))[,1],
  R2=as.data.frame(colMeans(r2_profile))[,1]
)
means$"R1 Rolling Mean" <- rollmean(means$R1, 5, fill="extend")
means$"R2 Rolling Mean" <- rollmean(means$R2, 5, fill="extend")
melted_vals = melt(
  means,
  id.vars = "position",
  measure.vars = c("R1", "R2", "R1 Rolling Mean", "R2 Rolling Mean")
)
p <- ggplot(melted_vals, aes(x=position, y=value)) + 
  geom_line() +
  facet_wrap(~ variable, ncol=2) +
  xlab("Base Position") +
  ylab("PHRED Score")
print(p)
```
With these plots, we can get a general idea of the per-base mean quality across all samples in the project. There are two plots for forward (R1) and reverse (R2) reads. The top plots ("R1" and "R2") are the per-base mean quality across all samples, and the bottom plots ("R1 Rolling Mean" and "R2 Rolling Mean") use a rolling mean that is based on a centered 5bp sliding window.

As expected the quality tails off in the R2 reads, but overall the quality is good.

## Plot Sample FASTQ Quality Profiles (Subset)
Below is an example of the quality profile for a single sample. At the end of this document you can find the quality profile for all `r length(sample_names)` samples available in this project.

```{r Sample Quality Profile, fig.width=12, fig.asp=0.25, message=FALSE, warning=FALSE}
print(plotQualityProfile(c(fastq_r1[1], fastq_r2[1])))
```

## Filter and Trim
For this project the V1 and V2 region of the 16s rRNA gene was sequenced. This region is roughly ~360bp and our project includes 2x250 Illumina MiSeq sequencing. For filtering and trimming we need to maintain a total length greater than 360bp plus a 20bp overlap, so ~380bp. Given 2x250bp reads, this gives 120bp to play with for trimming.

```{r filterAndTrim}
filt_r1 <- file.path(
  PROJECT_DIR, "data", "filtered", paste0(sample_names, "_R1_filt.fastq.gz")
)
filt_r2 <- file.path(
  PROJECT_DIR, "data", "filtered", paste0(sample_names, "_R2_filt.fastq.gz")
)
names(filt_r1) <- sample_names
names(filt_r2) <- sample_names
out <- as.data.frame(
  filterAndTrim(fastq_r1, filt_r1, fastq_r2, filt_r2, truncLen=c(250,210),
                minLen=190, maxN=0, maxEE=c(2,5), truncQ=2, rm.phix=TRUE,
                compress=TRUE, multithread=TRUE, verbose=FALSE)
)
filtered_stats <- out
colnames(filtered_stats) <- c("input", "filtered")
rownames(filtered_stats) <- sample_names
filtered_stats$percent_filtered_out <- 1 - (filtered_stats$filtered / filtered_stats$input)
```

Using the quality profile of the complete set of samples, the following parameters were using for `filterAndTrim` (fitering and trimming):

| Parameters            | Description                                                              |
|-----------------------|--------------------------------------------------------------------------|
| `truncLen=c(250,210)` | Trim length of forward (250bp) and reverse (210bp) reads                 |
| `minLen=185`          | Remove reads that are less than 190bp after trimming                     |
| `maxN=0`              | Filter reads containing ambiguous (e.g. N) base calls                    | 
| `maxEE=c(2,5)`        | Filter reads with more than expected basecall errors (R1: 2, R2: 5)      |
| `truncQ=2`            | Trim reads at the position with a PHRED score of Q2                      |
| `rm.phix=TRUE`        | Remove any reads matching PhiX (Illumina control)                        |
| `compress=TRUE`       | Gzip output FASTQs                                                       |
| `multithread=TRUE`    | Use multiple processors                                                  | 
| `verbose=FALSE`       | Do not output status messages                                            | 

```{r filterAndTrim Histogram, fig.width=12, message=FALSE, warning=FALSE}
print(histogram(
  filtered_stats$percent_filtered_out,
  nint=50,
  type="count",
  xlab="Percent of Reads Filtered Out"
))
summary(filtered_stats$percent_filtered_out)
```
Above is histogram and summary of the percentage of reads filtered out by `filterAndTrim` for each sample. Ideally it should be left-skewed, meaning most reads passed filtering and trimming and were not filtered out. If you find most of the reads are being filtered out, please review the `filterAndTrim` parameters used above. In cases were a large percentage of reads were filtered out, manual review might be necessary to determine the cause.

Below is the top 10 samples with the most reads filtered out to review.
```{r filterAndTrim Top 10}
head(filtered_stats[order(filtered_stats$percent_filtered_out, decreasing=TRUE),], n=10)
```

## Determine and Plot Error Rates
```{r Determine Error Profile, fig.width=12, message=FALSE, warning=FALSE}
error_r1 <- learnErrors(filt_r1, multithread=TRUE, verbose=FALSE)
error_r2 <- learnErrors(filt_r2, multithread=TRUE, verbose=FALSE)
print(plotErrors(error_r1, nominalQ=TRUE))
```

In the plot above the dots are the observed error rates for each consensus quality score. The redline shows the expected error rates based and the black line fit to the observed error rates.

## Sample Inference (Denoise) and Merge Reads
```{r Denoise and Merge, fig.width=12, message=FALSE, warning=FALSE}
dada_r1 <- dada(filt_r1, err=error_r1, multithread=TRUE, verbose=FALSE)
dada_r2 <- dada(filt_r2, err=error_r2, multithread=TRUE, verbose=FALSE)
merged_reads <- mergePairs(dada_r1, filt_r1, dada_r2, filt_r2, verbose=TRUE)
print(histogram(
  sapply(merged_reads, getN) / filtered_stats$filtered,
  nint=50,
  endpoints=c(0,1),
  type="count",
  xlab="Percent of Reads Merged"
))
summary(sapply(merged_reads, getN) / filtered_stats$filtered)
```

For the above histogram (and summary stats), we are expecting it to be right-skewed, that is to say most of the read pairs were merged. If the percentage of read pairs merged is low across all samples, it might be necessary to review the parameters used with `filterAndTrim`. By default, `mergePairs` requires only a 12bp overlap to merge read pairs. Please keep this in mind if it does become necessary to adjust `filterAndTrim` parameters. 

## Construct Amplicon Sequence Variant (ASV) Table
```{r Construct ASV Table, fig.width=12, message=FALSE, warning=FALSE}
original_sequence_table <- makeSequenceTable(merged_reads)
print(histogram(
  nchar(getSequences(original_sequence_table)), 
  nint=50, 
  type="count",
  xlab="Sequence Length"
))
summary(nchar(getSequences(original_sequence_table)))
```

The above histogram (and summary stats), is showing the distribution of ASV sequence lengths for the merged reads. For this project, V1 and V2 was sequenced which is expected to be around ~360bp. In our case we have ASV lengths spanning from `r min(nchar(getSequences(original_sequence_table)))`bp to `r max(nchar(getSequences(original_sequence_table)))`bp.

Since some of these ASVs could be the result of non-specific priming, ASVs with lengths more than 20bp from the expected 360bp will be filtered out.
```{r Construct ASV Table Filter, fig.width=12, message=FALSE, warning=FALSE}
sequence_table <- original_sequence_table[,nchar(colnames(original_sequence_table)) %in% 340:380]

print(histogram(
  nchar(getSequences(sequence_table)), 
  nint=50, 
  type="count",
  xlab="Sequence Length"
))
summary(nchar(getSequences(sequence_table)))
```

After filtering ASVs based on sequence lengths, we can see in the above histogram (and summary stats) all our ASVs fall within 20bp of the expected 360bp. Before filtering there were `r dim(original_sequence_table)[[2]]` ASVs, now after filtering there are `r dim(sequence_table)[[2]]` ASVs remaining.


## Remove Chimeras
```{r Remove Chimeras, message=FALSE, warning=FALSE}
sequence_table_nochim <- removeBimeraDenovo(
  sequence_table, method="consensus", multithread=TRUE, verbose=TRUE
)
```

During the process of removing chimeras, it is likely for a lot of ASVs to drop out. That is ok. What is important is the abundance that these chimeras account for. In our case, `r dim(sequence_table_nochim)[[2]]` ASVs, or `r (dim(sequence_table_nochim)[[2]] / dim(sequence_table)[[2]]) * 100`%, remain after identifying chimeras. These remaining ASVs account for `r (sum(sequence_table_nochim) / sum(sequence_table)) * 100`% of the merged reads and the ASVs marked as chimeras accounted for `r (1 - (sum(sequence_table_nochim) / sum(sequence_table))) * 100` of the merged reads.

If you find that a majority of the merged reads were filtered as chimeras, it may be necessary to revisit the parameters used with `filterAndTrim`.

## Track Reads
The following table will give an idea at which steps reads were lost throughout the analysis. Reviewing this table might be helpful to determine if it is necessary to revisit parameters used during the analysis.
```{r Track Reads, message=FALSE, warning=FALSE}
track <- as.data.frame(cbind(
  out,
  sapply(dada_r1, getN),
  sapply(dada_r2, getN),
  sapply(merged_reads, getN),
  rowSums(sequence_table_nochim)
))
colnames(track) <- c("input", "filtered", "denoisedR1", "denoisedR2", "merged", "nonchim")
rownames(track) <- sample_names
track$percent_filtered_out <- 1 - (track$filtered / track$input)
track$percent_merged <- track$merged / track$filtered
track$percent_nochim <- track$nonchim / track$merged
print(track)
```


## Assign Taxonomy
The [SILVA (v132)](https://zenodo.org/record/1172783#.XT7ZwpNKiL4) DADA2-formatted reference database was used to assign taxonomy (including species) to the ASVs. The SILVA (v132) DADA2 formatted databases are available at [DADA2 Reference Databases](https://benjjneb.github.io/dada2/training.html).
```{r Assign Taxonomy, message=FALSE, warning=FALSE}
taxa <- assignTaxonomy(
  sequence_table_nochim, 
  paste0(PROJECT_DIR, "/data/reference/silva_nr_v132_train_set.fa.gz"),
  tryRC=TRUE,
  multithread=TRUE
)
taxa <- addSpecies(
  taxa,
  paste0(PROJECT_DIR, "/data/reference/silva_species_assignment_v132.fa.gz")
)

asv_seqs <- colnames(sequence_table_nochim)
asv_headers <- vector(dim(sequence_table_nochim)[2], mode="character")
for (i in 1:dim(sequence_table_nochim)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep="_")
}
asv_fasta <- c(rbind(asv_headers, asv_seqs))
write(asv_fasta, paste0(PROJECT_DIR, "/results/ASVs-withcontams.fa"))

asv_tab <- t(sequence_table_nochim)
row.names(asv_tab) <- sub(">", "", asv_headers)
write.table(
  asv_tab,
  paste0(PROJECT_DIR, "/results/ASVs-withcontams-counts.tsv"),
  sep="\t",
  quote=F,
  col.names=NA
)

asv_tax <- taxa
row.names(asv_tax) <- sub(">", "", asv_headers)
write.table(
  asv_tax, 
  paste0(PROJECT_DIR, "/results/ASVs-withcontams-taxonomy.tsv"),
  sep="\t",
  quote=F,
  col.names=NA
)

taxa_print <- as.data.frame(taxa)
rownames(taxa_print) <- NULL
head(taxa_print)
```

Taxonomy was assigned to `r nrow(taxa_print)` ASVs, of these, `r nrow(taxa_print[!is.na(taxa_print$Species), ])` also had a species assigned. The results of these assignments were written to the following files:

**ASV Sequences (FASTA)**  
Each ASV sequence was written to *results/ASVs-withcontams.fa* in FASTA format


**ASV Counts Per Sample**  
A TSV of the count for each ASV within each sample was written to *results/ASVs-withcontams-counts.tsv*


**Taxonomy Assignment Per ASV**  
The taxonomic assignment for each ASV (similar to the table above) was written to *results/ASVs-withcontams-taxonomy.tsv*


## Remove Contaminants
[decontam](https://github.com/benjjneb/decontam) is used to identify ASVs that are likely contaminants. decontam includes [two methods to identify contaminants](https://benjjneb.github.io/decontam/vignettes/decontam_intro.html), frequency and prevalence. In this analysis, the prevalence method is used. This method compares the *prevalence* (presence/absence across samples) of each ASV in true positive samples against negative controls (e.g. *BLANK* samples) to identify contaminants.

```{r decontam, message=FALSE, warning=FALSE}
vector_for_decontam <- grepl("BLANK", colnames(asv_tab))
contam_asvs <- isContaminant(t(asv_tab), neg=vector_for_decontam)
table(contam_asvs$contaminant)
is_contaminant <- row.names(contam_asvs[contam_asvs$contaminant == TRUE,])
asv_tax[row.names(asv_tax) %in% is_contaminant, ]
contam_indices <- which(asv_fasta %in% paste0(">", is_contaminant))
dont_want <- sort(c(contam_indices, contam_indices + 1))

asv_fasta_no_contam <- asv_fasta[- dont_want]
write(asv_fasta_no_contam, paste0(PROJECT_DIR, "/results/ASVs-decontam.fa"))

asv_tab_no_contam <- asv_tab[!row.names(asv_tab) %in% is_contaminant, ]
write.table(
  asv_tab_no_contam,
  paste0(PROJECT_DIR, "/results/ASVs-decontam-counts.tsv"),
  sep="\t",
  quote=F,
  col.names=NA
)

asv_tax_no_contam <- asv_tax[!row.names(asv_tax) %in% is_contaminant, ]
write.table(
  asv_tax_no_contam,
  paste0(PROJECT_DIR, "/results/ASVs-decontam-taxonomy.tsv"),
  sep="\t",
  quote=F,
  col.names=NA
)
```


`r nrow(contam_asvs[contam_asvs$contaminant == TRUE,])` ASVs were identified as contaminants and their taxonmic assignments were filtered out. The decontaminated results were written to the following files:

**ASV Sequences (FASTA)**  
Each decontaminated ASV sequence was written to *results/ASVs-decontam.fa* in FASTA format


**ASV Counts Per Sample**  
A TSV of the count for each decontaminated ASV within each sample was written to *results/ASVs-decontam-counts.tsv*


**Taxonomy Assignment Per ASV**  
The taxonomic assignment for each decontaminated ASV (similar to the table above) was written to *results/ASVs-decontam-taxonomy.tsv*


## Plot Sample FASTQ Quality Profiles (Full Set)
```{r Full Set Quality Profile, fig.width=12, fig.asp=0.25, message=FALSE, warning=FALSE}
for (i in 1:length(fastq_r1)) {
  print(plotQualityProfile(c(fastq_r1[i], fastq_r2[i])))
}
```
